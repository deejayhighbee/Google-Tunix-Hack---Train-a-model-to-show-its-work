{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":5029473,"sourceType":"datasetVersion","datasetId":2918309}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ayodejiibrahimlateef/tunix-hack-on-tpu-grpo?scriptVersionId=291562570\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Google Tunix Hackathon — Train Gemma3 to “Show Its Work”","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os, json, time, re, glob\nimport numpy as np\nfrom pathlib import Path\n\nimport jax\nfrom jax.sharding import Mesh\n\nfrom kaggle_secrets import UserSecretsClient\nfrom flax import nnx\nimport qwix\n\nfrom huggingface_hub import snapshot_download","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:54:36.31397Z","iopub.execute_input":"2026-01-13T02:54:36.314266Z","iopub.status.idle":"2026-01-13T02:54:36.317499Z","shell.execute_reply.started":"2026-01-13T02:54:36.314237Z","shell.execute_reply":"2026-01-13T02:54:36.316755Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Fresh Start","metadata":{}},{"cell_type":"code","source":"import shutil, os\nCKPT_DIR = \"/kaggle/working/ckpts\"\nshutil.rmtree(CKPT_DIR, ignore_errors=True)\nos.makedirs(CKPT_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:54:36.320147Z","iopub.execute_input":"2026-01-13T02:54:36.32034Z","iopub.status.idle":"2026-01-13T02:54:36.330355Z","shell.execute_reply.started":"2026-01-13T02:54:36.320319Z","shell.execute_reply":"2026-01-13T02:54:36.329649Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## HF_TOKEN (Kaggle Secrets) + sanity checks","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nif not HF_TOKEN or not isinstance(HF_TOKEN, str) or len(HF_TOKEN.strip()) < 10:\n    raise RuntimeError(\n        \"HF_TOKEN missing/invalid.\\n\"\n        \"Kaggle Notebook → Add-ons → Secrets → add HF_TOKEN.\\n\"\n        \"Also accept the Gemma license on Hugging Face for your account.\"\n    )\n\nos.environ[\"HF_HOME\"] = str(Path(\"/kaggle/working\") / \"hf_home\")\nos.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n\nprint(\"HF_TOKEN ok (length):\", len(HF_TOKEN))\nprint(\"HF_HOME:\", os.environ[\"HF_HOME\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:54:36.330881Z","iopub.execute_input":"2026-01-13T02:54:36.331049Z","iopub.status.idle":"2026-01-13T02:54:36.415041Z","shell.execute_reply.started":"2026-01-13T02:54:36.331034Z","shell.execute_reply":"2026-01-13T02:54:36.414021Z"}},"outputs":[{"name":"stdout","text":"HF_TOKEN ok (length): 37\nHF_HOME: /kaggle/working/hf_home\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## NNX/qwix compat patch","metadata":{}},{"cell_type":"code","source":"from flax.nnx import variablelib as _variablelib\n\nif not getattr(_variablelib.Variable.set_metadata, \"_patched_for_qwix\", False):\n    _orig_set_metadata = _variablelib.Variable.set_metadata\n\n    def _set_metadata_compat(self, *args, **kwargs):\n        if len(args) == 2 and not kwargs and isinstance(args[0], str):\n            return _orig_set_metadata(self, **{args[0]: args[1]})\n        return _orig_set_metadata(self, *args, **kwargs)\n\n    _set_metadata_compat._patched_for_qwix = True\n    _variablelib.Variable.set_metadata = _set_metadata_compat\n\nprint(\"NNX metadata compat patch: OK\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:54:36.415799Z","iopub.execute_input":"2026-01-13T02:54:36.415974Z","iopub.status.idle":"2026-01-13T02:54:36.420485Z","shell.execute_reply.started":"2026-01-13T02:54:36.415957Z","shell.execute_reply":"2026-01-13T02:54:36.419675Z"}},"outputs":[{"name":"stdout","text":"NNX metadata compat patch: OK\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Download Gemma3 1B (HF gated) to local folder","metadata":{}},{"cell_type":"code","source":"MODEL_ID = \"google/gemma-3-1b-it\"\nLOCAL_DIR = str(Path(\"/kaggle/working/gemma_local/gemma-3-1b-it\"))\n\ntry:\n    local_model_path = snapshot_download(\n        repo_id=MODEL_ID,\n        token=HF_TOKEN,\n        local_dir=LOCAL_DIR,\n        local_dir_use_symlinks=False,\n    )\nexcept Exception as e:\n    raise RuntimeError(\n        \"Failed to download Gemma from Hugging Face.\\n\"\n        \"Common causes:\\n\"\n        \"- You have not accepted the Gemma license on Hugging Face\\n\"\n        \"- Your HF_TOKEN is wrong / expired\\n\"\n        \"- Kaggle internet is OFF\\n\"\n        f\"\\nRaw error:\\n{repr(e)}\"\n    )\n\nprint(\"local_model_path:\", local_model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:54:36.420892Z","iopub.execute_input":"2026-01-13T02:54:36.421056Z","iopub.status.idle":"2026-01-13T02:54:38.436764Z","shell.execute_reply.started":"2026-01-13T02:54:36.421042Z","shell.execute_reply":"2026-01-13T02:54:38.435848Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ee5711d68f4bdabdde981cbe66bc63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed43f549e0534f4e90f1a2345a923505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.68k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a57fb405a7be4b2ab8ce78b1d781fb18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"953846bbbaf148a5b60f7dfa4e994633"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fd5432205bd4769b36349cf574284d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/24.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"387ca469bfd4405590a16fa39f2a4eb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f368d752101947a8a995d5bcb41a6146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"066482791f124686a7af9a15dd1626d3"}},"metadata":{}},{"name":"stdout","text":"local_model_path: /kaggle/working/gemma_local/gemma-3-1b-it\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Tokenizer + EOS_TOKENS","metadata":{}},{"cell_type":"code","source":"from tunix.generate import tokenizer_adapter as tokenizer_lib\n\ntokenizer_path = os.path.join(local_model_path, \"tokenizer.model\")\nassert os.path.exists(tokenizer_path), f\"Missing tokenizer.model at: {tokenizer_path}\"\n\ntokenizer = tokenizer_lib.Tokenizer(tokenizer_path=tokenizer_path)\n\nEOS_TOKENS = []\ngen_cfg_path = os.path.join(local_model_path, \"generation_config.json\")\nif os.path.exists(gen_cfg_path):\n    with open(gen_cfg_path, \"r\", encoding=\"utf-8\") as f:\n        gen_cfg = json.load(f)\n    eos = gen_cfg.get(\"eos_token_id\", None)\n    if eos is None:\n        eos = gen_cfg.get(\"eos_token_ids\", None)\n    if eos is not None:\n        EOS_TOKENS = eos if isinstance(eos, list) else [int(eos)]\n\n# Always include tokenizer eos_id\nEOS_TOKENS.append(int(tokenizer.eos_id()))\nEOS_TOKENS = list(dict.fromkeys([int(x) for x in EOS_TOKENS]))\n\nprint(\"tokenizer.eos_id():\", int(tokenizer.eos_id()))\nprint(\"EOS_TOKENS:\", EOS_TOKENS)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:54:38.437405Z","iopub.execute_input":"2026-01-13T02:54:38.43757Z","iopub.status.idle":"2026-01-13T02:54:48.345205Z","shell.execute_reply.started":"2026-01-13T02:54:38.437555Z","shell.execute_reply":"2026-01-13T02:54:48.344128Z"}},"outputs":[{"name":"stdout","text":"tokenizer.eos_id(): 1\nEOS_TOKENS: [1, 106]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Define Mesh","metadata":{}},{"cell_type":"code","source":"devices = jax.devices()\nassert len(devices) == 8, f\"Expected 8 TPU devices, got {len(devices)}\"\n\nMESH_SHAPE = (2, 4)\nAXIS_NAMES = (\"fsdp\", \"tp\") \n\nmesh = Mesh(np.array(devices).reshape(MESH_SHAPE), AXIS_NAMES)\nprint(\"Mesh:\", mesh)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:54:48.34588Z","iopub.execute_input":"2026-01-13T02:54:48.346207Z","iopub.status.idle":"2026-01-13T02:54:59.556557Z","shell.execute_reply.started":"2026-01-13T02:54:48.346191Z","shell.execute_reply":"2026-01-13T02:54:59.555405Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1768272891.421780      12 common_lib.cc:648] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:238\n","output_type":"stream"},{"name":"stdout","text":"Mesh: Mesh('fsdp': 2, 'tp': 4, axis_types=(Auto, Auto))\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Load base model (Gemma3 1B) from safetensors","metadata":{}},{"cell_type":"code","source":"from tunix.models.gemma3 import model as gemma_lib\nfrom tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n\nmodel_config = gemma_lib.ModelConfig.gemma3_1b()\n\nwith mesh:\n    base_model = params_safetensors_lib.create_model_from_safe_tensors(\n        local_model_path, model_config, mesh\n    )\n\nprint(\"Base model loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:54:59.557157Z","iopub.execute_input":"2026-01-13T02:54:59.557326Z","iopub.status.idle":"2026-01-13T02:55:02.539089Z","shell.execute_reply.started":"2026-01-13T02:54:59.55731Z","shell.execute_reply":"2026-01-13T02:55:02.537982Z"}},"outputs":[{"name":"stdout","text":"Base model loaded.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Build LoRA policy (qwix)","metadata":{}},{"cell_type":"code","source":"RANK = 64\nALPHA = 64.0\n\ndef make_lora_policy(base_model):\n    lora_provider = qwix.LoraProvider(\n        module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum\",\n        rank=RANK,\n        alpha=ALPHA,\n    )\n    model_input = base_model.get_model_input()\n    lora_model = qwix.apply_lora_to_model(\n        base_model,\n        lora_provider,\n        rngs=nnx.Rngs(0),\n        **model_input,\n    )\n\n    state = nnx.state(lora_model)\n    pspecs = nnx.get_partition_spec(state)\n    state = jax.lax.with_sharding_constraint(state, pspecs)\n    nnx.update(lora_model, state)\n    return lora_model\n\nwith mesh:\n    lora_policy = make_lora_policy(base_model)\n\nprint(\"LoRA policy created.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:55:02.539737Z","iopub.execute_input":"2026-01-13T02:55:02.539902Z","iopub.status.idle":"2026-01-13T02:55:16.179476Z","shell.execute_reply.started":"2026-01-13T02:55:02.539887Z","shell.execute_reply":"2026-01-13T02:55:16.178342Z"}},"outputs":[{"name":"stdout","text":"LoRA policy created.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Prompt builder (two lines enforced)","metadata":{}},{"cell_type":"code","source":"SYSTEM_PROMPT = (\n    \"You must respond with EXACTLY two lines and nothing else:\\n\"\n    \"<reasoning>...</reasoning>\\n\"\n    \"<answer>...</answer>\\n\\n\"\n    \"Rules:\\n\"\n    \"- The first line MUST start with <reasoning> and end with </reasoning>\\n\"\n    \"- The second line MUST start with <answer> and end with </answer>\\n\"\n    \"- Do not write 'Reasoning:' or any other text outside the tags.\\n\"\n)\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\ndef make_prompt(question: str) -> str:\n    return TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:55:16.180251Z","iopub.execute_input":"2026-01-13T02:55:16.180435Z","iopub.status.idle":"2026-01-13T02:55:16.183709Z","shell.execute_reply.started":"2026-01-13T02:55:16.180418Z","shell.execute_reply":"2026-01-13T02:55:16.18294Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Sampler smoke test (compiles sampler, short output)","metadata":{}},{"cell_type":"code","source":"from tunix.generate import sampler as sampler_lib\n\nMAX_PROMPT_LENGTH = 256\nSMOKE_GEN_STEPS = 256\n\nsampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + SMOKE_GEN_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n\nq = \"If John has 3 apples and buys 4 more, how many apples does he have?\"\nprompt = make_prompt(q)\n\nt0 = time.time()\nout = sampler(\n    input_strings=[prompt],\n    max_generation_steps=SMOKE_GEN_STEPS,\n    max_prompt_length=MAX_PROMPT_LENGTH,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.95,\n    echo=False,\n    eos_tokens=EOS_TOKENS,\n)\nprint(\"Warmup generate seconds:\", time.time() - t0)\nprint(out.text[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:55:16.1841Z","iopub.execute_input":"2026-01-13T02:55:16.184258Z","iopub.status.idle":"2026-01-13T02:58:27.248711Z","shell.execute_reply.started":"2026-01-13T02:55:16.184243Z","shell.execute_reply":"2026-01-13T02:58:27.247721Z"}},"outputs":[{"name":"stdout","text":"Warmup generate seconds: 190.98067426681519\n<answer>11</answer>\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## GSM8K load from /kaggle/input (JSONL), parse answers","metadata":{}},{"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/gsm8k-dataset\")\nTRAIN_JSONL = DATA_DIR / \"train.jsonl\"\nTEST_JSONL  = DATA_DIR / \"test.jsonl\"\n\nassert TRAIN_JSONL.exists(), f\"Missing: {TRAIN_JSONL}\"\nassert TEST_JSONL.exists(), f\"Missing: {TEST_JSONL}\"\n\ndef read_jsonl(path: Path, limit: int | None = None):\n    rows = []\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for i, line in enumerate(f):\n            line = line.strip()\n            if not line:\n                continue\n            rows.append(json.loads(line))\n            if limit is not None and len(rows) >= limit:\n                break\n    return rows\n\ndef pick_question_answer(row: dict):\n\n    q = (row.get(\"question\") or row.get(\"query\") or row.get(\"prompt\") or row.get(\"input\") or \"\")\n    a = (row.get(\"answer\") or row.get(\"output\") or row.get(\"response\") or row.get(\"label\") or \"\")\n    \n    if not q and isinstance(row.get(\"text\"), str):\n        q = row[\"text\"]\n    if not a and isinstance(row.get(\"target\"), str):\n        a = row[\"target\"]\n\n    return str(q).strip(), str(a).strip()\n\nHASH_ANS_RE = re.compile(r\"####\\s*([-+]?\\d+(?:\\.\\d+)?)\\s*$\")\n\ndef extract_hash_answer(ans: str):\n    m = HASH_ANS_RE.search(ans.strip())\n    if m:\n        return m.group(1).strip()\n    m2 = re.findall(r\"[-+]?\\d+(?:\\.\\d+)?\", ans)\n    return m2[-1].strip() if m2 else ans.strip()\n\nSYSTEM_PROMPT = (\n    \"You must respond with EXACTLY two lines and nothing else:\\n\"\n    \"<reasoning>...</reasoning>\\n\"\n    \"<answer>...</answer>\\n\\n\"\n    \"Rules:\\n\"\n    \"- The first line MUST start with <reasoning> and end with </reasoning>\\n\"\n    \"- The second line MUST start with <answer> and end with </answer>\\n\"\n    \"- Do not write 'Reasoning:' or any other text outside the tags.\\n\\n\"\n    \"Example:\\n\"\n    \"<reasoning>We add the numbers.</reasoning>\\n\"\n    \"<answer>7</answer>\"\n)\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\ndef make_prompt(question: str) -> str:\n    return TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)\n\ndef build_records(rows, limit: int | None = None):\n    recs = []\n    for r in rows[:limit] if limit is not None else rows:\n        q, a = pick_question_answer(r)\n        if not q:\n            continue\n        gold = extract_hash_answer(a) if a else None\n        recs.append({\"prompts\": make_prompt(q), \"answer\": gold})\n    return recs\n\n# Load + build\ntrain_rows = read_jsonl(TRAIN_JSONL, limit=4096)   # tune up/down for runtime\nval_rows   = read_jsonl(TEST_JSONL,  limit=256)\n\ntrain_records = build_records(train_rows)\nval_records   = build_records(val_rows)\n\nprint(\"train_records:\", len(train_records), \"val_records:\", len(val_records))\nprint(\"\\nFirst prompt preview:\\n\", train_records[0][\"prompts\"][:500])\nprint(\"\\nFirst gold answer:\", train_records[0][\"answer\"])\nprint(\"\\nHas full </answer> tag in prompt header?\", \"</answer>\" in train_records[0][\"prompts\"].split(\"<end_of_turn>\")[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:58:27.249254Z","iopub.execute_input":"2026-01-13T02:58:27.249433Z","iopub.status.idle":"2026-01-13T02:58:27.488114Z","shell.execute_reply.started":"2026-01-13T02:58:27.249417Z","shell.execute_reply":"2026-01-13T02:58:27.487095Z"}},"outputs":[{"name":"stdout","text":"train_records: 4096 val_records: 256\n\nFirst prompt preview:\n <start_of_turn>user\nYou must respond with EXACTLY two lines and nothing else:\n<reasoning>...</reasoning>\n<answer>...</answer>\n\nRules:\n- The first line MUST start with <reasoning> and end with </reasoning>\n- The second line MUST start with <answer> and end with </answer>\n- Do not write 'Reasoning:' or any other text outside the tags.\n\nExample:\n<reasoning>We add the numbers.</reasoning>\n<answer>7</answer>\n\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in Ma\n\nFirst gold answer: 72\n\nHas full </answer> tag in prompt header? True\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Records + batch generators (object dtype)","metadata":{}},{"cell_type":"code","source":"def infinite_batches(records, batch_size: int, seed: int = 0):\n    rng = np.random.default_rng(seed)\n    idx = np.arange(len(records))\n    while True:\n        rng.shuffle(idx)\n        for i in range(0, len(idx) - batch_size + 1, batch_size):\n            batch = [records[j] for j in idx[i:i+batch_size]]\n            prompts = np.array([str(b[\"prompts\"]) for b in batch], dtype=object)\n            answers = np.array([None if b[\"answer\"] is None else str(b[\"answer\"]) for b in batch], dtype=object)\n            yield {\"prompts\": prompts, \"answer\": answers}\n\ndef finite_batches(records, batch_size: int):\n    for i in range(0, len(records) - batch_size + 1, batch_size):\n        batch = records[i:i+batch_size]\n        prompts = np.array([str(b[\"prompts\"]) for b in batch], dtype=object)\n        answers = np.array([None if b[\"answer\"] is None else str(b[\"answer\"]) for b in batch], dtype=object)\n        yield {\"prompts\": prompts, \"answer\": answers}\n\n\nTRAIN_MICRO_BATCH_SIZE = 2\ntrain_dataset = infinite_batches(train_records, TRAIN_MICRO_BATCH_SIZE, seed=42)\nval_dataset   = finite_batches(val_records, TRAIN_MICRO_BATCH_SIZE)\n\nb = next(train_dataset)\nprint(\"prompt type:\", type(b[\"prompts\"][0]), \"answer type:\", type(b[\"answer\"][0]))\nprint(\"prompt preview:\", b[\"prompts\"][0][:120])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:58:27.488537Z","iopub.execute_input":"2026-01-13T02:58:27.488691Z","iopub.status.idle":"2026-01-13T02:58:27.494309Z","shell.execute_reply.started":"2026-01-13T02:58:27.488676Z","shell.execute_reply":"2026-01-13T02:58:27.493537Z"}},"outputs":[{"name":"stdout","text":"prompt type: <class 'str'> answer type: <class 'str'>\nprompt preview: <start_of_turn>user\nYou must respond with EXACTLY two lines and nothing else:\n<reasoning>...</reasoning>\n<answer>...</an\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Reward functions","metadata":{}},{"cell_type":"code","source":"from decimal import Decimal, InvalidOperation\n\nFORMAT_RE = re.compile(\n    r\"^\\s*<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\\s*$\",\n    re.DOTALL\n)\n\nTWO_LINE_RE = re.compile(\n    r\"^\\s*<reasoning>.*?</reasoning>\\n<answer>.*?</answer>\\s*$\",\n    re.DOTALL\n)\n\ndef _extract_between(text: str, start: str, end: str):\n    try:\n        i = text.index(start) + len(start)\n        j = text.index(end, i)\n        return text[i:j].strip()\n    except ValueError:\n        return None\n\ndef _normalize_number(s: str):\n    s = s.strip()\n    try:\n        return Decimal(s)\n    except (InvalidOperation, ValueError):\n        m = re.search(r\"[-+]?\\d+(\\.\\d+)?\", s)\n        return Decimal(m.group(0)) if m else None\n\ndef r_starts_with_reasoning(prompts, completions, **kwargs):\n    r = []\n    for c in completions:\n        c = \"\" if c is None else str(c)\n        r.append(1.5 if c.lstrip().startswith(\"<reasoning>\") else -1.5)\n    return np.array(r, dtype=np.float32)\n\ndef r_answer_tags_present(prompts, completions, **kwargs):\n    r = []\n    for c in completions:\n        c = \"\" if c is None else str(c)\n        has_open = \"<answer>\" in c\n        has_close = \"</answer>\" in c\n        if has_open and has_close:\n            r.append(1.0)\n        elif has_open or has_close:\n            r.append(-1.0)\n        else:\n            r.append(-0.5)\n    return np.array(r, dtype=np.float32)\n\ndef r_second_line_starts_with_answer(prompts, completions, **kwargs):\n    r = []\n    for c in completions:\n        c = \"\" if c is None else str(c)\n        lines = c.strip(\"\\n\").splitlines()\n        if len(lines) < 2:\n            r.append(-2.0)\n            continue\n        r.append(2.0 if lines[1].lstrip().startswith(\"<answer>\") else -3.0)\n    return np.array(r, dtype=np.float32)\n\ndef r_format_soft(prompts, completions, **kwargs):\n    r = []\n    for c in completions:\n        c = \"\" if c is None else str(c)\n        score = 0.0\n        score += 0.5 if \"<reasoning>\" in c else -0.5\n        score += 0.5 if \"</reasoning>\" in c else -0.5\n        score += 0.5 if \"<answer>\" in c else -0.5\n        score += 0.5 if \"</answer>\" in c else -0.5\n        score -= 1.0 if \"Reasoning:\" in c else 0.0\n        score -= 1.0 if \"<start_of_turn>\" in c else 0.0\n        score -= 1.0 if \"<end_of_turn>\" in c else 0.0\n        r.append(score)\n    return np.array(r, dtype=np.float32)\n\ndef r_format_strict(prompts, completions, **kwargs):\n    r = []\n    for c in completions:\n        c = \"\" if c is None else str(c)\n        r.append(6.0 if TWO_LINE_RE.match(c) else -3.0)\n    return np.array(r, dtype=np.float32)\n\ndef r_answer_exact(prompts, completions, **kwargs):\n    gt = kwargs.get(\"answer\", None)\n    r = []\n    for i, c in enumerate(completions):\n        c = \"\" if c is None else str(c)\n        gold = None if gt is None else gt[i]\n        if gold is None:\n            r.append(0.0)\n            continue\n\n        if FORMAT_RE.match(c) is None:\n            r.append(-1.0)\n            continue\n\n        pred_raw = _extract_between(c, \"<answer>\", \"</answer>\")\n        if pred_raw is None:\n            r.append(-1.0)\n            continue\n\n        pred_num = _normalize_number(pred_raw)\n        gold_num = _normalize_number(str(gold))\n\n        if pred_num is not None and gold_num is not None:\n            r.append(4.0 if pred_num == gold_num else -1.0)\n        else:\n            r.append(2.0 if pred_raw.strip() == str(gold).strip() else -1.0)\n\n    return np.array(r, dtype=np.float32)\n\nprint(\"Reward functions ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:58:27.494735Z","iopub.execute_input":"2026-01-13T02:58:27.494896Z","iopub.status.idle":"2026-01-13T02:58:27.508408Z","shell.execute_reply.started":"2026-01-13T02:58:27.494881Z","shell.execute_reply":"2026-01-13T02:58:27.507657Z"}},"outputs":[{"name":"stdout","text":"Reward functions ready.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## GRPO setup + preflight + warmup + debug train + train","metadata":{}},{"cell_type":"code","source":"import optax\nfrom orbax import checkpoint as ocp\n\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.sft import metrics_logger\nimport time\n\nCKPT_DIR = str(Path(\"/kaggle/working/ckpts\"))\n\nMAX_STEPS = 200\nEVAL_EVERY_N_STEPS = 200\nROLLOUT_TOKENS = 128\nNUM_GENERATIONS = 2\nassert TRAIN_MICRO_BATCH_SIZE >= NUM_GENERATIONS, (\n    \"TRAIN_MICRO_BATCH_SIZE must be >= NUM_GENERATIONS for GRPO.\"\n)\n\nLEARNING_RATE = 3e-6\nWARMUP_STEPS = max(1, int(0.1 * MAX_STEPS))\n\nlr_schedule = optax.schedules.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=LEARNING_RATE,\n    warmup_steps=WARMUP_STEPS,\n    decay_steps=MAX_STEPS,\n    end_value=0.0,\n)\n\noptimizer = optax.chain(\n    optax.clip_by_global_norm(0.1),\n    optax.adamw(lr_schedule, b1=0.9, b2=0.99, weight_decay=0.1),\n)\n\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=25,\n    max_to_keep=3,\n)\n\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=str(Path(\"/kaggle/working/tensorboard/grpo\")),\n    flush_every_n_steps=10,\n)\n\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine=\"vanilla\",\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        metrics_logging_options=metrics_logging_options,\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=ROLLOUT_TOKENS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=MAX_PROMPT_LENGTH + ROLLOUT_TOKENS + 256,\n        temperature=0.9,\n        top_p=1.0,\n        top_k=50,\n        eos_tokens=EOS_TOKENS,\n    ),\n)\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=1,\n    beta=0.08,\n    epsilon=0.2,\n)\n\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=base_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\ntrainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    algo_config=grpo_config,\n    reward_fns=[\n        r_starts_with_reasoning,\n        r_answer_tags_present,\n        r_second_line_starts_with_answer,\n        r_format_soft,\n        r_format_strict,\n        r_answer_exact,\n    ],\n)\n\ndef log(msg):\n    print(time.strftime(\"[%H:%M:%S]\"), msg)\n\nlog(\"Preflight: fetching one batch...\")\nb = next(infinite_batches(train_records, TRAIN_MICRO_BATCH_SIZE, seed=123))\nlog(f\"Batch ok. prompt_type={type(b['prompts'][0])}, answer_type={type(b['answer'][0])}\")\n\nlog(\"Preflight: tokenizing first prompt...\")\n_ = tokenizer.encode(str(b[\"prompts\"][0]))\nlog(\"Tokenizer ok.\")\n\nlog(\"Warmup: rl_cluster.generate ...\")\nt0 = time.time()\n_ = rl_cluster.generate([str(b[\"prompts\"][0])], mode=\"train\", micro_batch_size=1)\nlog(f\"rl_cluster.generate seconds: {time.time() - t0:.3f}\")\n\nlog(\"Debug train: 2 steps, skip_jit=True ...\")\nt0 = time.time()\nwith mesh:\n    trainer.train(infinite_batches(train_records, TRAIN_MICRO_BATCH_SIZE, seed=999), finite_batches(val_records, TRAIN_MICRO_BATCH_SIZE), skip_jit=True)\nlog(f\"debug train seconds: {time.time() - t0:.3f}\")\n\nlog(\"Train: full run ...\")\nt0 = time.time()\nwith mesh:\n    trainer.train(train_dataset, val_dataset, skip_jit=False)\nlog(f\"train seconds: {time.time() - t0:.3f}\")\n\nprint(\"Done. Checkpoints:\", CKPT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:58:27.508821Z","iopub.execute_input":"2026-01-13T02:58:27.508964Z"}},"outputs":[{"name":"stderr","text":"WARNING:absl:Reference model and actor model are colocated but do not share the same backbone. This will result in an unnecessary model copy and increased HBM usage.\n","output_type":"stream"},{"name":"stdout","text":"[02:58:29] Preflight: fetching one batch...\n[02:58:29] Batch ok. prompt_type=<class 'str'>, answer_type=<class 'str'>\n[02:58:29] Preflight: tokenizing first prompt...\n[02:58:29] Tokenizer ok.\n[02:58:29] Warmup: rl_cluster.generate ...\n[03:01:28] rl_cluster.generate seconds: 178.904\n[03:01:28] Debug train: 2 steps, skip_jit=True ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Actor Training:   0%|          | 0/200 [00:00<?, ?step/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e998c5a78d8e499391f1eaadab4d0f3d"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"## Post-train quick evaluation (3 examples)","metadata":{}},{"cell_type":"code","source":"EVAL_GENERATION_STEPS = 990\n\neval_sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + EVAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n\nexamples = [str(val_records[i][\"prompts\"]) for i in range(3)]\n\nout = eval_sampler(\n    input_strings=examples,\n    max_generation_steps=EVAL_GENERATION_STEPS,\n    max_prompt_length=MAX_PROMPT_LENGTH,\n    temperature=0.5,\n    top_k=50,\n    top_p=0.9,\n    echo=False,\n    eos_tokens=EOS_TOKENS,\n)\n\nfor i, t in enumerate(out.text):\n    print(f\"\\n--- sample {i} ---\\n{t[:2900]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save an artifact of the checkpoint","metadata":{}},{"cell_type":"code","source":"import shutil, os\nfrom pathlib import Path\n\nassert os.path.exists(CKPT_DIR), f\"CKPT_DIR not found: {CKPT_DIR}\"\n\nzip_path = \"/kaggle/working/ckpts.zip\"\nif Path(zip_path).exists():\n    Path(zip_path).unlink()\n\nshutil.make_archive(\"/kaggle/working/ckpts\", \"zip\", CKPT_DIR)\nprint(\"Saved:\", zip_path)\n\nprint(\"\\n/kaggle/working contents:\")\nfor name in sorted(os.listdir(\"/kaggle/working\")):\n    p = Path(\"/kaggle/working\") / name\n    if p.is_file():\n        print(f\"{p}  ({p.stat().st_size/1024/1024:.1f} MB)\")\n    else:\n        print(f\"{p}/\")\n\nprint(\"\\nSome checkpoint files:\")\ncount = 0\nfor p in Path(CKPT_DIR).rglob(\"*\"):\n    if p.is_file():\n        print(p)\n        count += 1\n        if count >= 40:\n            break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}